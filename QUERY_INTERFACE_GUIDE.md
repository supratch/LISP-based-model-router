# ğŸ¯ Query Routing Interface - User Guide

## Overview

The Query Routing Interface now provides **complete end-to-end visibility** from query submission to LLM response generation.

---

## ğŸ–¥ï¸ Interface Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Query Routing Interface                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  LEFT PANEL: Query Input              RIGHT PANEL: Results         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Query Text Box          â”‚          â”‚ ğŸ¤– Generated Response   â”‚ â”‚
â”‚  â”‚ (Multi-line)            â”‚          â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚                         â”‚          â”‚ â”‚ [LLM Response Text] â”‚ â”‚ â”‚
â”‚  â”‚                         â”‚          â”‚ â”‚                     â”‚ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ â”‚ Generation: 2.34s   â”‚ â”‚ â”‚
â”‚                                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  Priority: [Medium â–¼]                 â”‚                         â”‚ â”‚
â”‚  Source EID: [10.1.0.1]               â”‚ ğŸ“Š Routing Result       â”‚ â”‚
â”‚  Preferred Model: [Optional]          â”‚ â€¢ Model: llama3.2-local â”‚ â”‚
â”‚                                       â”‚ â€¢ Method: LISP + DNS    â”‚ â”‚
â”‚  [Route Query Button]                 â”‚ â€¢ Confidence: 92%       â”‚ â”‚
â”‚                                       â”‚ â€¢ Endpoint: 127.0.0.1   â”‚ â”‚
â”‚                                       â”‚                         â”‚ â”‚
â”‚                                       â”‚ ğŸ” Technical Details    â”‚ â”‚
â”‚                                       â”‚ [Expandable Accordion]  â”‚ â”‚
â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Step-by-Step Usage

### 1. Enter Your Query

**Location**: Left panel, top text box

**Example Queries**:
- "Explain machine learning in simple terms"
- "Write a Python function to sort a list"
- "What is the capital of France?"
- "Generate a creative story about space exploration"

**Tips**:
- Be specific for better routing
- Longer queries get better model selection
- Code queries â†’ routed to Phi-3
- Creative queries â†’ routed to Llama 3.2

---

### 2. Configure Options

**Priority** (Dropdown):
- `Low` - Best effort, may use slower models
- `Medium` - Balanced performance (default)
- `High` - Fastest available model

**Source EID** (Text field):
- Default: `10.1.0.1`
- Represents your logical network identity
- Used for LISP routing decisions

**Preferred Model** (Optional):
- Leave blank for automatic selection
- Or specify: `llama3.2-local`, `phi3-local`, etc.

---

### 3. Submit Query

Click the **"Route Query"** button

**What Happens**:
1. â³ Button shows "Routing..." with spinner
2. ğŸ”„ Backend performs routing analysis
3. ğŸ¤– Selected model generates response
4. âœ… Results appear in right panel

**Timing**:
- Routing decision: ~50-200ms
- LLM generation: 1-5 seconds (depends on query)
- Total: Usually 2-6 seconds

---

## ğŸ“Š Understanding the Results

### Section 1: Generated Response (NEW! âœ¨)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¤– Generated Response from llama3.2-local          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Machine learning is a subset of artificial        â”‚
â”‚  intelligence that enables computers to learn      â”‚
â”‚  from data without being explicitly programmed.    â”‚
â”‚  It uses algorithms to identify patterns and       â”‚
â”‚  make predictions or decisions based on that       â”‚
â”‚  data...                                           â”‚
â”‚                                                     â”‚
â”‚  Generation time: 2.34s                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Features**:
- âœ… Shows actual LLM-generated text
- âœ… Displays which model generated it
- âœ… Shows generation time
- âœ… Scrollable for long responses
- âœ… Monospace font for readability

---

### Section 2: Routing Result Cards

Three cards showing key metrics:

**Card 1: Selected Model**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ§  Model      â”‚
â”‚ llama3.2-local  â”‚
â”‚ Selected Model  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Card 2: Routing Method**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ”€ Router     â”‚
â”‚  LISP + DNS     â”‚
â”‚ Routing Method  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Card 3: Response Time**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   âš¡ Speed      â”‚
â”‚     1.20s       â”‚
â”‚ Est. Response   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Section 3: Routing Details

**Confidence Score**:
- ğŸŸ¢ Green (80-100%): High confidence
- ğŸŸ¡ Yellow (60-79%): Medium confidence
- ğŸ”´ Red (<60%): Low confidence

**Endpoint**: Physical server address (RLOC)
- `127.0.0.1` = Local model
- `192.168.x.x` = Remote server

**Estimated Cost**: Per-query cost
- `$0.000000` = Free (local models)
- `$0.00003` = Paid API (GPT-4, Claude)

**Reasoning**: Why this model was selected
- Example: "Selected Llama 3.2 for general query with good reasoning capabilities"

---

### Section 4: Alternative Models

Shows other models that could handle the query:

```
Alternative Models:
[phi3-local] [gpt-4] [claude-3]
```

Click to see why they weren't selected.

---

### Section 5: Technical Metadata (Expandable)

Click to expand and see:

**Query Analysis**:
- Query type (e.g., "SIMPLE_QA", "CODE_GENERATION")
- Complexity score
- Estimated tokens
- Language detected

**LISP Routing**:
- Source EID â†’ Destination EID mapping
- RLOC addresses
- Packet ID
- Instance ID

**DNS Routing**:
- Service name
- DNS record used
- Address and port

**Performance**:
- Processing time in milliseconds

---

## ğŸ¨ Visual Indicators

### Loading State
```
[Routing... â³]
```
- Button disabled
- Spinner animation
- Previous results cleared

### Success State
```
âœ… Results displayed
ğŸ¤– Response generated
ğŸ“Š Metrics shown
```

### Error State
```
âŒ Error: [Error message]
```
- Red alert box
- Descriptive error message
- Suggestions for resolution

---

## ğŸ’¡ Tips & Best Practices

### For Best Results

1. **Be Specific**: "Write a Python function to sort" â†’ Better than "code"
2. **Use Context**: Include relevant details in your query
3. **Check Model**: See which model was selected and why
4. **Review Response**: Verify the generated answer makes sense

### Model Selection Guide

| Query Type | Best Model | Why |
|------------|------------|-----|
| Code generation | `phi3-local` | Optimized for code |
| Math problems | `phi3-local` | Excellent at math |
| Creative writing | `llama3.2-local` | Better creativity |
| General Q&A | `llama3.2-local` | Balanced performance |
| Complex reasoning | `llama3.2-local` | Better reasoning |

---

## ğŸ”§ Troubleshooting

### No Response Generated

**Symptom**: Error message instead of response

**Check**:
1. Is Ollama running? `curl http://localhost:11434/api/tags`
2. Are models installed? `ollama list`
3. Check backend logs for errors

### Slow Response

**Symptom**: Takes >10 seconds

**Solutions**:
- Use smaller models (3B instead of 7B)
- Simplify your query
- Check system resources (CPU/RAM)

### Wrong Model Selected

**Symptom**: Unexpected model chosen

**Solutions**:
- Use "Preferred Model" field to force selection
- Make query more specific
- Check routing reasoning in results

---

## ğŸš€ Quick Examples

### Example 1: Code Generation
```
Query: "Write a Python function to calculate fibonacci numbers"
Expected Model: phi3-local
Expected Response: Python code with explanation
```

### Example 2: General Question
```
Query: "What is machine learning?"
Expected Model: llama3.2-local
Expected Response: Clear explanation with examples
```

### Example 3: Creative Writing
```
Query: "Write a short story about a robot learning to paint"
Expected Model: llama3.2-local
Expected Response: Creative narrative
```

---

**Ready to try it?** Open http://localhost:3000 and start querying! ğŸ‰

